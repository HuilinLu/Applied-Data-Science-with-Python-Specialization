{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Some of the cells in this notebook are computationally expensive. To reduce runtime, this notebook is using a subset of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification (Supervised Learning for Text)\n",
    "### What is Classification\n",
    "#### Given a set of classes\n",
    "#### Classification (Task): ssign the correct class label to the given input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Text Classification\n",
    "#### Topic identification: Is this news article about Politics, Sports, or Technology?\n",
    "#### Spam Detection: Is this email a spam or not?\n",
    "#### Sentiment analysis: Is this movie review positive or negative?\n",
    "#### Spelling correction: weather or whether? color or colour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "#### Humans learn from past experiences, machines learn from past instances!\n",
    "Model built(Classification Algorithm) with the labeled input in the training phase and apply 'Classification Model' in the Inference Phase on the unlabeld input to get the outcomes.\n",
    "#### Learn a classification model on properties ('features') and their importance ('weights') from labeled instances.\n",
    "##### -- X: Set of attributes or features {x1, x2, ..., xn}, n features\n",
    "##### -- y: A 'class' label from the label set Y = {y1, y2, ..., yk}\n",
    "#### Apply the model on new instances to predict the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Classification: Phases and Datasets\n",
    "#### Training Phase(Labeled Data Set), separated into Training Dataset and Validation ('Hold Out') Data set(evaluate how well model is)\n",
    "#### Inference Phase: Unlabeled Data Set, set Test Data Set(Unseen Data at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Paradigms\n",
    "#### When there are only two possible classes; |Y| = 2 : Binary Classification\n",
    "#### When there are more than two possible classes; |Y| > 2: Multi-class Classification\n",
    "#### When data instances can have two or more labels: Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to ask in Supervised Learning\n",
    "#### --Training Phase:\n",
    "##### -- What are the features? How do you represent them?\n",
    "##### -- What is the classficiation model/algorithm?\n",
    "##### -- What are the model parameters?\n",
    "#### Inference Phase:\n",
    "##### -- What is the expected performance? What is a good measure? (evaluation metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Features from Text\n",
    "### Why  is textual data unique?\n",
    "#### Textual data presents a unique set of challenges.\n",
    "#### All the information you need is in the text.\n",
    "#### But features can be pulled out from text at different granularities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of textual features (I)\n",
    "#### Words\n",
    "---- By far the most common class of features.\n",
    "\n",
    "---- Handling commonly - occurring words: called 'Stop words' eg: the, a. It is less important for those stop words\n",
    "\n",
    "---- Normalization: Make lower case vs. leave as-is.\n",
    "\n",
    "---- Stemming / Lemmatization. eg: plurals don't be the different features.\n",
    "\n",
    "###  Types of textual features (2)\n",
    "#### Characteristics of words: Capitalization. eg:  us vs US, White House vs white house\n",
    "#### Parts of speech of words in a sentence. eg: whether or weather when have 'the' in front\n",
    "#### Grammatical structure, sentence parsing. eg. verb associated with a particular noun \n",
    "#### Grouping words of similar meaning, semantics. eg {buy, purhcase}, {Mr., Ms., Dr., Prof.}; Numbers / Digits; Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Textual Features (3)\n",
    "#### Depending on classification tasks, features may come from inside words and word sequence\n",
    "---- bigrams, trigrams, n-grams: 'White House'.\n",
    "\n",
    "---- character sub-sequences in words: 'ing'(verb), 'ion'(noun)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall lectures from previous week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "### Case Study: Classifying text search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose you are interested in classifying search queries in three classes: Entertainment, Computer Science, Zoology\n",
    "#### Most common class of the three is Entertainment. (Without knowing any other information)\n",
    "Circumstance 1:\n",
    "#### Suppose the query is 'Python', identify its class\n",
    "---- Python, the snake (Zoology).\n",
    "\n",
    "---- Python, the programming language (Computer Science).\n",
    "\n",
    "---- Python, as in Monty Python (ENtertainment).\n",
    "#### Most common class, given 'Python' is Zoology though Entertaiment has the largest probability\n",
    "Circumstance 2:\n",
    "#### Suppose the query is 'Python download'\n",
    "#### Most probable class, given 'Python download', is Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Model\n",
    "#### Update the likelihood of the class given new information\n",
    "#### Prior Probability: Pr(y=Entertainment), Pr(y=CS), Pr(y=Zoology)\n",
    "#### Posterior Probability: Pr(y=Entertainment|x='Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Rule\n",
    "#### Posterior Probability = Prior Probability * Likelihood / Evidence\n",
    "#### Pr(y|X) = Pr(y) *Pr(X|y)/ Pr(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "#### Pr(y = CS|'Python') = Pr(y=CS)*Pr('Python'|y=CS) / Pr('Python')\n",
    "#### Pr(y=Zoology|'Python') = Pr(y=Zoology)*Pr('Python'|y=Zoology) / Pr('Python')\n",
    "#### Pr(y=CS|'Python') > Pr(y=Zoology|'Python') then we predict the label is Computer Science y=CS\n",
    "Pr(y|X) = Pr(y) * Pr(X|y) / Pr(X)\n",
    "\n",
    "y* = argmax Pr(y|X) = argmax Pr(y)*Pr(X|y)\n",
    "#### Naive Assumption: Given the class label, features are assumed to be independent of each other.\n",
    "y* = argmax Pr(y|X) = argmax Pr(y) * pie Pr(xi|y) (i from 1 to n, first feature to the last feature)\n",
    "#### Query 'Python download'\n",
    "y* = argmax Pr(y|'Python download') = argmax Pr(y) * Pr('Python'|y) * Pr('download'|y)\n",
    "\n",
    "\n",
    "Thus, if y=CS, then it will maximize Pr(y|'Python download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: What are the parameters?\n",
    "#### Prior Probabilities: Pr(y) for all y in Y, (k classes, the sum is 1 and k-1 independent parameters)\n",
    "#### Likelihood: Pr(xi|y) for all features xi and labels y in Y\n",
    "#### If there are 3 classes (|Y| = 3) and 100 features in X(binary), how many parameters does Naive Bayes models have? Answer: 3 + 3*100*2 = 603 and 302 independent parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Learning Parameters\n",
    "#### Prior probabilities: Pr(y) for all y in Y\n",
    "---- Remember training data?\n",
    "\n",
    "---- Count the number of instances in each class.\n",
    "\n",
    "---- If there are N instances in all, and n out of those are labeled as class y, then Pr(y) = n/N\n",
    "#### Likelihood: Pr(xi|y) for all features xi and all labels y in Y\n",
    "---- Count how many times feature xi appears in instances labeled as class y.\n",
    "\n",
    "---- If there are p instances of class y, and xi appears in k of those, Pr(xi|y) = k/p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Smoothing\n",
    "#### What happens if Pr(xi|y) = 0? this feature does not appear in this class\n",
    "---- Feature xi never occurs in ducoments labeled y\n",
    "\n",
    "---- But then, the posterior probability Pr(y|xi) will be 0!!\n",
    "#### Instead, smooth parameters. We do not want to be so strict!\n",
    "#### Laplace smoothing or Additive smoothing: Add a dummy count\n",
    "---- Pr(xi|y) = (k+1) / (p+n); where n is number of features. Thus we will not have 0 probability now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Concepts\n",
    "#### Naive Bayes is a probabilistic model.\n",
    "#### Naive, because it assumes features are independent of each other, given the class label.\n",
    "#### For text classification problems, Naive Bayes models typically provide very strong baselines.\n",
    "#### Simple model, easy to learn parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Variations\n",
    "### Two Classic Naive Bayes Variants for Text\n",
    "#### Multinomial Naive Bayes\n",
    "---- Data follows a multinomial distribution.\n",
    "\n",
    "---- Each feature value is a count (word occurrence counts, TF-IDF(Terms Frequency, Inverse Document Frequency) weighting, ...)\n",
    "#### Bernoulli Naive Bayes\n",
    "---- Data follows a multivariate Bernoulli Distribution.\n",
    "\n",
    "---- Each feature is binary (word is present / absent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "### Classifier = Function on input data\n",
    "### Decision Boundary\n",
    "#### Classification function is represented by decision surfaces: How do you find them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Decision Boundary\n",
    "#### Data overfitting: Decision boundary learned over training data doesn't generalize well to test data.\n",
    "\n",
    "### Linear Boundaries\n",
    "In two dimension, it will be straight line; In three-dimension, it will be a plane; In high dimension, it will be a hyperplane.\n",
    "#### Advantage of Linear Boundaries: Easy to find, Easy to evaluate, More generalizable ('Occam's razor): Simple model generalize on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Linear Boundary\n",
    "#### Find the linear boundary = Find w(intercept)\n",
    "#### Many methods: Perceptron; Linear Discriminative Analysis; Linear Least Squares\n",
    "#### Problem: If linearly separable, then infinite number of linear boundaries!\n",
    "\n",
    "Some decision boundaries are not good because they are causing confusions, a small change in the data points will cause a big change in the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "#### What is a reasonable boundary?\n",
    "---- Margin\n",
    "\n",
    "---- Maximum-margin hyperplane: a small change in data points would still not change the label that this particular classifier will assign that.\n",
    "#### Use a lot of support vectors(confusion points) to find the maximum-margine hyperplane\n",
    "#### Support Vector Machines are maximum-margin classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "#### SVMs are linear classifiers (kernal='linear') that find a hyperplane to separate two classes of data: positive and negative.\n",
    "#### Given training data points (X1, y1), (X2, y2),...; where Xi = (x1, x2, ..., xn) features is instance vector and yi is one of {-1, +1}\n",
    "#### SVM finds a linear function w(weight vector), make:\n",
    "    f(Xi) = <w.Xi> + b  \n",
    "    if f(Xi) >= 0, yi = +1; else yi = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Multi-class classification\n",
    "#### SVMs work only for binary classification problems:\n",
    "    f(Xi) = <w.Xi> + b  \n",
    "    if f(Xi) >= 0, yi = +1; else yi = -1\n",
    "#### What about three classes?\n",
    "---- One vs Rest:\n",
    "    \n",
    "    n-class SVM has n classifiers\n",
    "---- One vs One:\n",
    "    \n",
    "    n-class SVM has C(n, 2) classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Parameters (I): Parameter C\n",
    "#### Regularization: How much importance should you give individual data points as compared to better generalized model.\n",
    "#### Regularization parameter C: (In Python: sklearn SVC)\n",
    "---- Larger values of C = less regularization:\n",
    "     \n",
    "     Fit training data as well as possible, every data point important\n",
    "---- Smaller values of C = more regularization:\n",
    "\n",
    "    More tolerant to errors on individual data points\n",
    "### SVM Parameters(2): Other params\n",
    "#### Linear kernels usually work best for text data: kernel = 'linear'\n",
    "---- Other kernels include rbf, polynomial\n",
    "#### multi_class: ovr (one-vs-rest)\n",
    "#### class_weight: Different classes can get different weights, eg: Spam(20%) vs Not Spam(80%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Messages\n",
    "#### Support Vector Machines tend to be the most accurate classifiers, especially in high-dimensional data\n",
    "#### Strong theoretical foundation: based on optimization theory.\n",
    "#### Handles only numeric features:\n",
    "    Convert categorical features to numeric features.\n",
    "    Normalization\n",
    "#### Hyperplane hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Text Classifiers in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkits for Supervised Text Classification\n",
    "#### Scikit-learn\n",
    "---- Open-source Machine Learning Library\n",
    "\n",
    "---- Started as Google Summer of Code by Dave Cournapeau, 2007\n",
    "\n",
    "---- Has a more programmatic interface\n",
    "#### NLTK\n",
    "---- Interfaces with sklearn and other ML toolkits (like Weka)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKlearn's NaiveBayesClassifier:\n",
    "    from sklearn import naive_bayes\n",
    "    clfrNB  = naive_bayes.MultinomialNB()\n",
    "    clfrNB.fit(x_train, y_train)\n",
    "    predicted_labels = clfrNB.predict(text_data)\n",
    "    metrics.f1_score(test_labels, predicted_labels, average = 'micro')\n",
    "### Using SKlearn's SVM Classifier:\n",
    "    from sklearn import Ssvm\n",
    "    clfrSVM = svm.SVC(kernel='linear', C=0.1)\n",
    "    clfSVM.fit(train_data, train_labels)\n",
    "    predicted_labels = clfrSVM.predict(test_data)\n",
    "### Model Selection\n",
    "#### Recall the discussion on multiple phases in a supervised learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection in Scikit-learn:\n",
    "---- 1. train test split:\n",
    "    \n",
    "    from sklearn import model_selection\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3333, random_state = 0)\n",
    "---- 2. cross validation:\n",
    "\n",
    "    predicted_labels = model_selection.cross_val_predict(clfrSVM, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Classification in NLTK\n",
    "#### NLTK has some classification algorithms:\n",
    "    NaiveBayesClassifier\n",
    "    DecisionTreeClassifier\n",
    "    ConditionalExponentialClassifier\n",
    "    MaxentClassifier\n",
    "    WekaClassifier\n",
    "    SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's NaiveBayesClassifier:\n",
    "    from nltk.classify import NaiveBayesClassifier\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    classifier.classify(unlabeled_instance)   ## only one instance\n",
    "    classifier.classify_many(unlabeled_instances)  ## many instances\n",
    "    nltk.classify.util.accuracy(classifier, test_set)\n",
    "    classifier.labels()\n",
    "    classifier.show_most_informative_features()     ## show the top 5/10 most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's SKlearnClassifier:\n",
    "    from nltk.classify import SklearnClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import SVC\n",
    "    cflrNB = SklearnClassifier(MultinomualNB()).train(train_data)\n",
    "    clfrSVM = SklearnClassifier(SVC(), kernal='linear').train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Concepts\n",
    "#### Scikit-learn most commonly used ML toolkit in Python\n",
    "#### NLTK has its own NaiveBayes implementation\n",
    "#### NLTK can also interface with Scikit-learn (and other ML toolkits like Weka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394349</th>\n",
       "      <td>Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.95</td>\n",
       "      <td>5</td>\n",
       "      <td>Very good one! Better than Samsung S and iphon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>The phone needed a SIM card, would have been n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248521</th>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I was 3 months away from my upgrade and my Str...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167661</th>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>an experience i want to forget</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT PHONE WORK ACCORDING MY EXPECTATIONS.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name   Price  \\\n",
       "394349  Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...        NaN  244.95   \n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless      Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...   Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...      CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...      Apple  922.00   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "394349       5  Very good one! Better than Samsung S and iphon...   \n",
       "34377        1  The phone needed a SIM card, would have been n...   \n",
       "248521       5  I was 3 months away from my upgrade and my Str...   \n",
       "167661       1                     an experience i want to forget   \n",
       "73287        5        GREAT PHONE WORK ACCORDING MY EXPECTATIONS.   \n",
       "\n",
       "        Review Votes  \n",
       "394349           0.0  \n",
       "34377            1.0  \n",
       "248521           3.0  \n",
       "167661           0.0  \n",
       "73287            1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "\n",
    "# Sample the data to speed up computation\n",
    "# Comment out this line to match with lecture\n",
    "df = df.sample(frac=0.1, random_state=10)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>Apple iPhone 5c 8GB (Pink) - Verizon Wireless</td>\n",
       "      <td>Apple</td>\n",
       "      <td>194.99</td>\n",
       "      <td>1</td>\n",
       "      <td>The phone needed a SIM card, would have been n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248521</th>\n",
       "      <td>Motorola Droid RAZR MAXX XT912 M Verizon Smart...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>174.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I was 3 months away from my upgrade and my Str...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167661</th>\n",
       "      <td>CNPGD [U.S. Office Extended Warranty] Smartwat...</td>\n",
       "      <td>CNPGD</td>\n",
       "      <td>49.99</td>\n",
       "      <td>1</td>\n",
       "      <td>an experience i want to forget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>Apple iPhone 7 Unlocked Phone 256 GB - US Vers...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>922.00</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT PHONE WORK ACCORDING MY EXPECTATIONS.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277158</th>\n",
       "      <td>Nokia N8 Unlocked GSM Touch Screen Phone Featu...</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>95.00</td>\n",
       "      <td>5</td>\n",
       "      <td>I fell in love with this phone because it did ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100311</th>\n",
       "      <td>Blackberry Torch 2 9810 Unlocked Phone with 1....</td>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>77.49</td>\n",
       "      <td>5</td>\n",
       "      <td>I am pleased with this Blackberry phone! The p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251669</th>\n",
       "      <td>Motorola Moto E (1st Generation) - Black - 4 G...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>89.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Great product, best value for money smartphone...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279878</th>\n",
       "      <td>OtterBox 77-29864 Defender Series Hybrid Case ...</td>\n",
       "      <td>OtterBox</td>\n",
       "      <td>9.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I've bought 3 no problems. Fast delivery.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406017</th>\n",
       "      <td>Verizon HTC Rezound 4G Android Smarphone - 8MP...</td>\n",
       "      <td>HTC</td>\n",
       "      <td>74.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone for the price...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302567</th>\n",
       "      <td>RCA M1 Unlocked Cell Phone, Dual Sim, 5Mp Came...</td>\n",
       "      <td>RCA</td>\n",
       "      <td>159.99</td>\n",
       "      <td>5</td>\n",
       "      <td>My mom is not good with new technoloy but this...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name  Brand Name   Price  \\\n",
       "34377       Apple iPhone 5c 8GB (Pink) - Verizon Wireless       Apple  194.99   \n",
       "248521  Motorola Droid RAZR MAXX XT912 M Verizon Smart...    Motorola  174.99   \n",
       "167661  CNPGD [U.S. Office Extended Warranty] Smartwat...       CNPGD   49.99   \n",
       "73287   Apple iPhone 7 Unlocked Phone 256 GB - US Vers...       Apple  922.00   \n",
       "277158  Nokia N8 Unlocked GSM Touch Screen Phone Featu...       Nokia   95.00   \n",
       "100311  Blackberry Torch 2 9810 Unlocked Phone with 1....  BlackBerry   77.49   \n",
       "251669  Motorola Moto E (1st Generation) - Black - 4 G...    Motorola   89.99   \n",
       "279878  OtterBox 77-29864 Defender Series Hybrid Case ...    OtterBox    9.99   \n",
       "406017  Verizon HTC Rezound 4G Android Smarphone - 8MP...         HTC   74.99   \n",
       "302567  RCA M1 Unlocked Cell Phone, Dual Sim, 5Mp Came...         RCA  159.99   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "34377        1  The phone needed a SIM card, would have been n...   \n",
       "248521       5  I was 3 months away from my upgrade and my Str...   \n",
       "167661       1                     an experience i want to forget   \n",
       "73287        5        GREAT PHONE WORK ACCORDING MY EXPECTATIONS.   \n",
       "277158       5  I fell in love with this phone because it did ...   \n",
       "100311       5  I am pleased with this Blackberry phone! The p...   \n",
       "251669       5  Great product, best value for money smartphone...   \n",
       "279878       5          I've bought 3 no problems. Fast delivery.   \n",
       "406017       4                       Great phone for the price...   \n",
       "302567       5  My mom is not good with new technoloy but this...   \n",
       "\n",
       "        Review Votes  Positively Rated  \n",
       "34377            1.0                 0  \n",
       "248521           3.0                 1  \n",
       "167661           0.0                 0  \n",
       "73287            1.0                 1  \n",
       "277158           0.0                 1  \n",
       "100311           0.0                 1  \n",
       "251669           0.0                 1  \n",
       "279878           0.0                 1  \n",
       "406017           0.0                 1  \n",
       "302567           4.0                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop missing values, drop any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (rated positively)\n",
    "# Encode 1s and 2s as 0 (rated poorly)\n",
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7471776686078667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most ratings are positive\n",
    "df['Positively Rated'].mean()  ## imbalanced data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
    "                                                    df['Positively Rated'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " Everything about it is awesome!\n",
      "\n",
      "\n",
      "X_train shape:  (23052,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "Bag-of-words approach: Converting a collection of text documents into a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " 'arroja',\n",
       " 'comapa√±ias',\n",
       " 'dvds',\n",
       " 'golden',\n",
       " 'lands',\n",
       " 'oil',\n",
       " 'razonable',\n",
       " 'smallsliver',\n",
       " 'tweak']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::2000]\n",
    "## This vocabulary is built on any tokens that occurred in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19601"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23052x19601 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 613289 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized\n",
    "## This representation is stored in a Scipy sparse matrix, where each row corresponds to a document and each column\n",
    "## a word from our training vocabulary\n",
    "## The entries in this matrix is the number of times each word appears in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8974332776669326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "## Note that any words in X_test that did not appear in X_train will just be ignored\n",
    "\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['worst' 'terrible' 'slow' 'junk' 'poor' 'sucks' 'horrible' 'useless'\n",
      " 'waste' 'disappointed']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' 'excellent' 'perfectly' 'love' 'perfect' 'exactly'\n",
      " 'great' 'best' 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf\n",
    "resscale the features, allows us to weight terms based on how important they are to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5442"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "## High weight is given to terms that appear often in a particular document, but do not appear often in the corpus\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)   ## min_df = parameter reduce the features that appear in a few documents only\n",
    "len(vect.get_feature_names())\n",
    "## Features with low tf-idf are either commonly used across all documents orrarely used and only occur in long documents\n",
    "## Features with high tf-idf are frequently used within specific documents, but rarely used accross all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.889951006492175\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "\n",
    "## Using far less features to get the same level of AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['61' 'printer' 'approach' 'adjustment' 'consequences' 'length' 'emailing'\n",
      " 'degrees' 'handsfree' 'chipset']\n",
      "\n",
      "Largest tfidf: \n",
      "['unlocked' 'handy' 'useless' 'cheat' 'up' 'original' 'exelent' 'exelente'\n",
      " 'exellent' 'satisfied']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'slow' 'disappointed' 'worst' 'terrible' 'never' 'return' 'doesn'\n",
      " 'horrible' 'waste']\n",
      "\n",
      "Largest Coefs: \n",
      "['great' 'love' 'excellent' 'good' 'best' 'perfect' 'price' 'awesome'\n",
      " 'far' 'perfectly']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# These reviews are treated the same by our current model\n",
    "## One problem with the bag-of-words approach is word order is disregarded\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))\n",
    "## Current model will regard this as a negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29072"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())\n",
    "## Adding bigram to our model, n-grams can be powerful in capturing meaning, longer sequences can cause an explosion of the number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9110661794597458\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "## Auc score increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'junk' 'poor' 'slow' 'worst' 'broken' 'not good' 'terrible'\n",
      " 'defective' 'horrible']\n",
      "\n",
      "Largest Coefs: \n",
      "['excellent' 'excelente' 'excelent' 'perfect' 'great' 'love' 'awesome'\n",
      " 'no problems' 'good' 'best']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# These reviews are now correctly identified\n",
    "print(model.predict(vect.transform(['not an issue, phone is working',\n",
    "                                    'an issue, phone is not working'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
