{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Text Similarity\n",
    "### Applications of semantic similarity\n",
    "#### Grouping similar words into semantic concepts\n",
    "#### As a building block in natural language understanding tasks\n",
    "---- Textual entailment: smaller sentence or sentence from a text document derives its meaning or entails its meaning from another pice of text.\n",
    "\n",
    "---- Paraphrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which pair of words are most similar\n",
    "---- a. deer, elk -- right choice, elk is one kind of deer\n",
    "\n",
    "---- b. deer, giraffe\n",
    "\n",
    "---- c. deer, horse\n",
    "\n",
    "---- d. deer, mouse\n",
    "##### What can be quantify this similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet (most extensive in English, and a few other languages).\n",
    "--- Resources useful for semantic similarity\n",
    "#### Semantic dictionary of (mostly) English words, interlinked by semantic relations\n",
    "---- Includes rich linguistic information: part of speech (verb or noun), word senses (different meanings of same word), synonyms, hypernyms/hyponyms (relationship: deer is a mammel), meronyms, derivationally related forms, ...\n",
    "#### Machine-readable, freely available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similartiy Using WordNet\n",
    "#### WordNet organizes information in a hierarchy\n",
    "#### Many similarity measures use the hierarchy in some way\n",
    "#### Verbs, nouns, adjectives all have separate hierarchies.\n",
    "eg: elk is under deer, deer and giraffe are siblings under ruminant, deer and horse are just relate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways of Finding Similarty between Concepts\n",
    "### 1.Path Similarity\n",
    "#### Find the shortest path between the two concepts \n",
    "#### Similarity measure inversely related to path distance\n",
    "PathSim(deer, elk) = 1/(distance + 1)= 1/(1+1) = 0.5\n",
    "\n",
    "PathSim(deer, giraffe) = 1/(distance + 1) = 1/(2+1) = 0.33\n",
    "\n",
    "PathSim(deer, horse) = 0.14 = 1/7\n",
    "### 2. Lowest Common Subsumer(LCS)\n",
    "#### Find the lowest (closest) ancestor to both concepts\n",
    "LCS(deer, elk) = deer\n",
    "\n",
    "LCS(deer, giraffe) = ruminant -- Though they have other ancestors, but we need to find the lowest ancestor in the hierarchy.\n",
    "\n",
    "LCS(deer, hourse) = ungulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin Similarity\n",
    "#### Similarity measure based on the information contained in the LCS of the two concepts\n",
    "#### LinSim(u, v) = 2 * log P(LCS(u,v)) / (log P(u) + log P(v))\n",
    "---- P(u) is given by the information content learnt over a large corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it in Python\n",
    "#### 1. WordNet easily imported into Python through NLTK\n",
    "#### 2. Find appropriate sense of the words\n",
    "#### 3. Find path similarity\n",
    "#### 4. Use an information criteria to find Lin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "### Find appropriate sense of the words\n",
    "deer = wn.synset('deer.n.01')   ## give me the first meaning of deer as a noun\n",
    "print(deer)\n",
    "\n",
    "elk = wn.synset('elk.n.01')\n",
    "print(elk)\n",
    "\n",
    "## Find path similarity\n",
    "deer.path_similarity(elk)  ## 0.5\n",
    "deer.path_similarity(horse)  ## 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Use an information criteria to find Lin similarity\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic(ic-brown.dat)\n",
    "\n",
    "deer.lin_similarity(elk, brown_ic)   ### 0.772\n",
    "deer.lin_similarity(horse, brown_ic)   ### 0.862\n",
    "\n",
    "## In WordNet hierarchy, deer and horse were very far awway, but LinSimilarity is very high, this is because in typical contexts\n",
    "## and the information that is contained by these words(deer and horse) are enough closer in similary(mammels).\n",
    "## Elk is a very specific instance of deer and not appear often in documents, thus LinSimilarity doesnot come out as close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations and Distributional Similarity\n",
    "#### 'You know a word by the company it keeps' [Firth, 1957]\n",
    "#### Two words that frequently appears in similar kind of contexts are more likely to be semantically related.\n",
    "eg:\n",
    "The friends met at a cafe.\n",
    "\n",
    "Shyam met Ray at a pizzeria.\n",
    "\n",
    "Let's meet up near the coffee shop.\n",
    "\n",
    "The secret meeting at the restaurant soon become public.\n",
    "\n",
    "\n",
    "Explain: cafe, pizzeria, coffee shop, restaurant are the words have semantic similarity, because they often appear around at 'near' and 'meet' words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional Similarity: Context\n",
    "#### Words before, after, within a small window\n",
    "before coffee shop -- 'a'      within a small window --- 'meet'\n",
    "#### Parts of speech of words before, after, in a small window\n",
    "#### Specific syntactic relation to the target word\n",
    "#### Words in the same sentence, same document, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strength of Association between Words\n",
    "#### How frequent are these?\n",
    "---- # Not similar if two words don't occur together often\n",
    "#### Also important to see how frequent are individual words\n",
    "---- # 'the' is very frequent, so high chances it co-occurs oftwn with every other word\n",
    "#### Thus we use Pointwise Mutual Information(Standardize)\n",
    "PMI(w, c) = log [P(w, c) / P(w)P(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it in Python\n",
    "#### 1. Use NLTK Collocations and Association Measures\n",
    "#### finder also has other useful functions, such as frequency filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measure = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text)   ## text is corpus, documentation\n",
    "finder.nbest(bigram_meaures.pmi, 10)  ## use PMI measure, get the top 10 pairs\n",
    "\n",
    "## finder pther useful functions, such as frequency filter\n",
    "finder.apply_freq_filter(10)\n",
    "## restrict any pair that occurs less than 10 times in corpus, keep words occurs more than 10 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Home Concepts\n",
    "#### Finding similarity between words and text is non-trivial\n",
    "#### WordNet is a useful resource for semantic relationships between words and semantic similarity\n",
    "#### Many similarity functions exist\n",
    "#### NLTK is a useful package for many such tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Documents exhibit multiple topics\n",
    "'Seeking Life's Bare (Genetic) Necessities\n",
    "#### Topic 1: Genetics: gene, sequence, genome,...\n",
    "#### Topic 2: Computation: number, computer, analysis, prediction\n",
    "#### Topic 3: Life sciences: life, survice, organism, ...\n",
    "### Intuition: Documents as a mixture of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is Topic Modeling?\n",
    "#### A coarse-level analysis of what's in a text collection.\n",
    "#### Topic: the subject (theme) of a discourse\n",
    "#### Topics are represented as a word distribution\n",
    "#### A document is assumed to be a mixture of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is Topic Modeling (2) ?\n",
    "#### What's known: The text collection or corpus; Number of topics\n",
    "#### What's not knowm: The actual topics, Topic distribution for each document.\n",
    "### What is Topic Modeling (3) ?\n",
    "#### Essentially, text clustering problem\n",
    "---- Documents and words clustered simultaneously\n",
    "\n",
    "---- Needs to figure out what words come together(similar, semantic similarity), also figure out what documents come together(same topic), how words derived from those documents, the distribution of words in a particular document, what is a probability of a word in a topic.\n",
    "#### Different topic modeling approaches available.\n",
    "---- Probabilistic Latent Semantic Analysis (PLSA) [Hoffman '99]\n",
    "\n",
    "---- Latent Dirichlet Allocation (LDA) [Blei, Ng, and Jordan, '03]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Generative Models and LDA\n",
    "### Generative Models for Text\n",
    "Generation: Use the model to generate the documents\n",
    "\n",
    "Inference, Estimation: Use the document to estimate the model, create the distribution of words\n",
    "\n",
    "Pr(text | model), eg Pr(the|model) = 0.1    Pr(is|model) = 0.07   Pr(harry |model) = 0.05    Pr(Potter|model)=0.04\n",
    "\n",
    "Explain: 1.generation story: we have one topic model, and then pull out words from the topic model to create document.\n",
    "\n",
    "2. Inference, Estimation to get the Pr(text | model)\n",
    "\n",
    "It can be complex, one documents can be inferenced with four models (mixture model) -- four distributions of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "#### Generative model for a document d :\n",
    "---- 1. Choose length of document d\n",
    "\n",
    "---- 2. Choose a mixture of topics for document d\n",
    "\n",
    "---- 3. Use a topic's multinomial distribution to output words to fill that topic's quota.\n",
    "\n",
    "eg: Suppose a particular document d, 40% of the words from topic A, then use the topic A's multinomial distribution to output the 40% of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Topic Modeling in Practice\n",
    "#### How many topics?\n",
    "---- Finding or even guessing the number of topics is hard.  (choice by yourself)\n",
    "#### Interpreting topics\n",
    "---- Topics are just word distributions.\n",
    "\n",
    "---- Making sense of words / generating labels for that topic is a subjective decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Topic Modeling: Summary\n",
    "#### Great tool for exploratory text analysis\n",
    "---- What are the documents (tweete, reviews, news articles) about?\n",
    "#### Many tools available to do it effortlessly in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Working with LDA in Python\n",
    "#### Many packages available, such as gensim, lda\n",
    "#### Step 1: Pre-processing text before using the packages\n",
    "---- Tokenize, normalize (lowercase)\n",
    "\n",
    "---- Stop word removal\n",
    "\n",
    "---- Stemming\n",
    "#### Step 2: Convert tokenized documents to a document - term matrix\n",
    "#### Step 3: Build LDA models on the doc-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Practice LDA in Python:\n",
    "#### doc_set : set of pre-processed text documents\n",
    "#### ldamodel can also be used to find topic distribution of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(doc_set)  ## dictionary mapping id and words\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_set]   ## create document term matrix\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 4, id2word = dictionary, passes = 50)\n",
    "## id2word, mapping study two words ahead\n",
    "print(ldamodel.print_topics(num_topics = 4, num_words = 5))  ## give the five top words in each of these four topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Take Home Concepts\n",
    "#### Topic modeling is an exploratory tool frequently used for text mining\n",
    "#### Latent Dirichlet Allocation is a generative modle used extensively for modeling large text corpora.\n",
    "#### LDA can also be used as a feature selection technique for text classification and other task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Information Extraction\n",
    "\n",
    "### Goal: Identify and extract fields of interest from free text\n",
    "Fields of interest: Title, Author, Reviewer, Published time, Place\n",
    "\n",
    "### Fields of Interest\n",
    "#### Named entities:\n",
    "---- [NEWS] People, Places, Dates, Organizations, ...\n",
    "\n",
    "---- [FINANCE] Money, Companies, ...\n",
    "\n",
    "---- [MEDICINE] Diseases, Drugs, Procedures, ...\n",
    "#### Relations\n",
    "---- What happened to who, when, where, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Information is hidden in free-text\n",
    "#### Most traditional transactional information is structured\n",
    "#### Currently 80% of data is Abundance of unstructured, freeform text.\n",
    "#### How to convert unstructured text to structured form? -- extract useful information from unstructured text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Named Entity Recognition\n",
    "#### Named entities: Noun phrases that are of specific type and refer to specific individuals, places, organizations, ...\n",
    "#### Named Entity Recognition: Technique(s) to identify all mentions of pre-defined named entities in text\n",
    "---- Identify the mention / phrase : Boundary detection (find start and end position)\n",
    "\n",
    "---- Identify the type: Tagging / classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Approaches to identify named entities\n",
    "#### Depends on kinds of entities that need to be identifies.\n",
    "#### For well-formatted fields like date, phone numbers: use Regular Expressions\n",
    "#### For other fields: Typically a machine learning approach, identify the words into different types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Person, Organization, Location/GPE\n",
    "#### Standard NER task in NLP research community\n",
    "#### Typically a four-class model:\n",
    "---- 1. PER\n",
    "\n",
    "---- 2. ORG\n",
    "\n",
    "---- 3. LOC / GPE\n",
    "\n",
    "---- 4. Other / Outside (any other class).\n",
    "\n",
    "John met Brendon. Jon and Brendon are PER, met is Outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relation extraction\n",
    "#### Identify relationships between named entities.\n",
    "eg: Erbitux helps treat lung cancer. Erbitux(treatment) and lung cancer(disease) are two named entities.\n",
    "\n",
    "Relationship: Ervitux is a treatment for lung cancer.\n",
    "\n",
    "### Co-reference resolution\n",
    "#### Disambiguate mentions and group mentions together.\n",
    "eg: Anita met Joseph at the market. He surprised her with a rose.\n",
    "\n",
    "Anita and Joseph are two named entities, he refers to Joseph and her refers to Anita.\n",
    "\n",
    "In this case, it is pronoun resolution where making a reference\n",
    "\n",
    "### Question Answering (why important)\n",
    "#### Given a question, find the most appropriate answer from the text.\n",
    "---- eg. What does Erbitux treat?\n",
    "\n",
    "First need to identify Erbitux is a treatment, relation is treat.\n",
    "\n",
    "---- eg. Who gave Anita the rose?\n",
    "#### Builds on named entity recognition, relation extraction, and co-reference resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Take Home Concepts\n",
    "#### Information Extraction is important for natural language understanding and making sense of textual data.\n",
    "#### Named Entity Recognition is a key  building block to address many advanced NLP tasks.\n",
    "#### Named Entity Recognition systems extensively deploy supervised machine learning and text mining techniques discussed in this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
